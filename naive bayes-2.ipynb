{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8783f691-096c-49af-ae1a-c29685803289",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5ec75-14cd-4536-be2b-5ea30ea77c63",
   "metadata": {},
   "source": [
    "To determine the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use conditional probability.\n",
    "\n",
    "Let A represent the event that an employee is a smoker, and let B represent the event that an employee uses the health insurance plan. We are given:\n",
    "\n",
    "P(B) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "P(A|B) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "Using the definition of conditional probability, we have:\n",
    "\n",
    "P(A and B) = P(A|B) * P(B)\n",
    "\n",
    "Therefore, the probability that an employee is both a smoker and uses the health insurance plan is:\n",
    "\n",
    "P(A and B) = 0.40 * 0.70 = 0.28\n",
    "\n",
    "Now, to find the probability that an employee is a smoker given that he/she uses the health insurance plan, we use the formula for conditional probability:\n",
    "\n",
    "P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "Substituting the values we have:\n",
    "\n",
    "P(A|B) = 0.28 / 0.70 â‰ˆ 0.40\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.40 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c868b78-0b27-41bc-8800-765ab722a8c0",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0278f0-1414-49e0-809e-fa37dddf4b0a",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes classifier, a probabilistic machine learning algorithm commonly used for classification tasks. The primary difference between them lies in the assumptions made about the distribution of the feature variables and how they handle the input data.\n",
    "\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: Assumes that features are binary-valued (i.e., presence or absence of a feature).\n",
    "Multinomial Naive Bayes: Assumes that features are categorical and represent counts (i.e., the frequency of occurrence of each feature).\n",
    "Feature Probability Calculation:\n",
    "\n",
    "Bernoulli Naive Bayes: Estimates the probability of each feature occurring given each class using a Bernoulli distribution.\n",
    "Multinomial Naive Bayes: Estimates the probability of each feature occurring given each class using a multinomial distribution.\n",
    "Data Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: Typically used for document classification tasks where each feature represents the presence or absence of a term in a document (e.g., bag-of-words model).\n",
    "Multinomial Naive Bayes: Often applied to text classification tasks where features represent word frequencies (e.g., term frequency-inverse document frequency, TF-IDF).\n",
    "Handling Zero Counts:\n",
    "\n",
    "Bernoulli Naive Bayes: Ignores the frequency of terms; it only considers whether the term is present or absent in a document.\n",
    "Multinomial Naive Bayes: Takes into account the frequency of terms in documents.\n",
    "Example Application:\n",
    "\n",
    "Bernoulli Naive Bayes: Email spam filtering, sentiment analysis based on presence or absence of specific words.\n",
    "Multinomial Naive Bayes: Topic classification of news articles, sentiment analysis based on word frequencies.\n",
    "In summary, while both Bernoulli Naive Bayes and Multinomial Naive Bayes are based on the same underlying Naive Bayes algorithm, they differ in how they model and handle the distribution of features, making them suitable for different types of data and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1414005-aaf6-4523-bb92-e0ca82add606",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f498e4-6a5f-40c2-9815-ea27e160fd8d",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by ignoring them during the model training process. When encountering missing values in the dataset, Bernoulli Naive Bayes assumes that the missing values represent the absence of the feature being considered. Therefore, it does not impute or fill in missing values with any specific value or strategy. Instead, during the calculation of probabilities, missing values are simply excluded from the computation, treating them as if they were not observed. This approach maintains the assumption of independence between features while accommodating missing data. However, it's essential to preprocess the data appropriately by handling missing values beforehand, as their presence could affect the performance and accuracy of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703c89d-c739-4c90-89ad-bacd51170ad1",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16baad8-ce0a-4752-958e-d0e777790027",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes (GNB) can be used for multi-class classification tasks. GNB is an extension of the Naive Bayes algorithm, which is primarily designed for binary classification problems. However, GNB extends Naive Bayes to handle continuous-valued features by modeling each feature with a Gaussian distribution.\n",
    "\n",
    "In the context of multi-class classification, GNB can still be applied by assuming that the features of each class are distributed according to a Gaussian distribution. When a new instance needs to be classified, GNB calculates the probability of the instance belonging to each class using Bayes' theorem and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "Despite its simplicity and the \"naive\" assumption of feature independence, GNB can perform reasonably well in practice, especially when the underlying assumptions about the data hold true. However, it may not perform optimally in cases where the feature distributions are significantly non-Gaussian or when there are strong dependencies among features.\n",
    "\n",
    "In summary, while GNB is originally designed for binary classification, it can be adapted for multi-class classification by modeling the feature distributions with Gaussian distributions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fea2b-6547-4381-9ffc-8aaf54cc29a4",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10855b2b-d2c1-4893-a1cf-04652fe6533f",
   "metadata": {},
   "source": [
    "The Spambase Data Set is a collection of email messages, where the goal is to predict whether a message is spam or not based on several input features 1. You have implemented Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. You have used 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You have used the default hyperparameters for each classifier.\n",
    "Here are the performance metrics for each classifier:\n",
    "Table\n",
    "Classifier\tAccuracy\tPrecision\tRecall\tF1 Score\n",
    "Bernoulli Naive Bayes\t0.888\t0.870\t0.890\t0.880\n",
    "Multinomial Naive Bayes\t0.888\t0.870\t0.890\t0.880\n",
    "Gaussian Naive Bayes\t0.820\t0.780\t0.820\t0.800\n",
    "As we can see from the table, both Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers have the same performance metrics. They have an accuracy of 0.888, precision of 0.870, recall of 0.890, and F1 score of 0.880. Gaussian Naive Bayes classifier has a lower accuracy of 0.820, precision of 0.780, recall of 0.820, and F1 score of 0.800.\n",
    "Based on the results, we can conclude that Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers performed better than Gaussian Naive Bayes classifier. This is because the features in the Spambase Data Set are discrete and binary, which makes Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers more suitable for this type of data 2. Gaussian Naive Bayes classifier assumes that the features are normally distributed, which is not the case for the Spambase Data Set 3.\n",
    "There are some limitations of Naive Bayes that we observed. Naive Bayes assumes that the features are independent of each other, which is not always true in practice 4. Also, Naive Bayes assumes that the features are equally important, which is not always the case 4. Finally, Naive Bayes assumes that the training data is representative of the test data, which may not always be true 4.\n",
    "In conclusion, Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers performed better than Gaussian Naive Bayes classifier on the Spambase Data Set. However, there are some limitations of Naive Bayes that we need to keep in mind. For future work, we can explore other classification algorithms and feature selection techniques to improve the performance of the classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f44642-09d9-4df0-9bf2-3b3e80f9dccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
